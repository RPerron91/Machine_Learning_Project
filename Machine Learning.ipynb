{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify predictions as boolean values\n",
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Preprocess values and remove future predictions column\n",
    "def preprocess_df(df):\n",
    "    df = df.drop('future', 1)\n",
    "    \n",
    "    # Drop non-numeric rows and change prices to percentage change values, then scale each column\n",
    "    for col in df.columns:\n",
    "        if col != \"target\":\n",
    "            df[col] = df[col].pct_change()\n",
    "            df.dropna(inplace=True)\n",
    "            df[col] = preprocessing.scale(df[col].values)\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Sort values into sequential chunks before randomizing for more accurate model predictions\n",
    "    sequential_data = []\n",
    "    prev_days = deque(maxlen=SEQ_LEN)\n",
    "    \n",
    "    for i in df.values:\n",
    "        prev_days.append([n for n in i[:-1]])\n",
    "        if len(prev_days) == SEQ_LEN:\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])\n",
    "            \n",
    "    # Randomize the sequence\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    buys = []\n",
    "    sells = []\n",
    "    \n",
    "    # If future price is greater than current price, buy, if future price less than current price, sell\n",
    "    for seq, target in sequential_data:\n",
    "        if target == 0:\n",
    "            sells.append([seq, target])\n",
    "        elif target == 1:\n",
    "            buys.append([seq, target])\n",
    "    \n",
    "    # Randomize each buy or sell\n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "    \n",
    "    # Balance buys/sells by creating equal length lists for buys/sells\n",
    "    # Use the lowest length value of the two lists as constraints\n",
    "    lower = min(len(buys), len(sells))\n",
    "    \n",
    "    buys = buys[:lower]\n",
    "    sells = sells[:lower]\n",
    "    \n",
    "    \n",
    "    sequential_data = buys+sells\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "        \n",
    "    return np.array(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 268 validation: 22\n",
      "Dont buys: 134, buys: 134\n",
      "TEST Dont buys: 11, buys: 11\n",
      "Train on 268 samples, validate on 22 samples\n",
      "Epoch 1/10\n",
      "268/268 [==============================] - 8s 29ms/sample - loss: 0.8707 - acc: 0.5299 - val_loss: 0.6940 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "268/268 [==============================] - 2s 6ms/sample - loss: 0.7799 - acc: 0.5821 - val_loss: 0.6940 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "268/268 [==============================] - 1s 6ms/sample - loss: 0.6693 - acc: 0.6754 - val_loss: 0.6944 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "268/268 [==============================] - 2s 6ms/sample - loss: 0.6101 - acc: 0.6530 - val_loss: 0.6943 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "268/268 [==============================] - 1s 2ms/sample - loss: 0.6181 - acc: 0.7276 - val_loss: 0.6940 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "268/268 [==============================] - 0s 2ms/sample - loss: 0.5706 - acc: 0.7201 - val_loss: 0.6938 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "268/268 [==============================] - 0s 2ms/sample - loss: 0.5301 - acc: 0.7463 - val_loss: 0.6939 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "268/268 [==============================] - 1s 4ms/sample - loss: 0.5256 - acc: 0.7388 - val_loss: 0.6943 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "268/268 [==============================] - 1s 2ms/sample - loss: 0.5255 - acc: 0.7388 - val_loss: 0.6950 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "268/268 [==============================] - 1s 3ms/sample - loss: 0.4271 - acc: 0.8172 - val_loss: 0.6954 - val_acc: 0.5000\n",
      "[1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1.]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "train data: 268 validation: 22\n",
      "Dont buys: 134, buys: 134\n",
      "TEST Dont buys: 11, buys: 11\n",
      "Train on 268 samples, validate on 22 samples\n",
      "Epoch 1/10\n",
      "268/268 [==============================] - 9s 33ms/sample - loss: 1.0179 - acc: 0.5261 - val_loss: 0.6927 - val_acc: 0.5909\n",
      "Epoch 2/10\n",
      "268/268 [==============================] - 1s 5ms/sample - loss: 0.7113 - acc: 0.5858 - val_loss: 0.6928 - val_acc: 0.5455\n",
      "Epoch 3/10\n",
      "268/268 [==============================] - 1s 2ms/sample - loss: 0.6752 - acc: 0.6306 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "268/268 [==============================] - 1s 3ms/sample - loss: 0.6261 - acc: 0.6679 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "268/268 [==============================] - 1s 2ms/sample - loss: 0.6308 - acc: 0.6754 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "268/268 [==============================] - 2s 6ms/sample - loss: 0.5583 - acc: 0.6828 - val_loss: 0.6940 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "268/268 [==============================] - 1s 2ms/sample - loss: 0.5252 - acc: 0.7463 - val_loss: 0.6940 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "268/268 [==============================] - 1s 5ms/sample - loss: 0.5396 - acc: 0.7127 - val_loss: 0.6938 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "268/268 [==============================] - 1s 2ms/sample - loss: 0.4588 - acc: 0.7910 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "268/268 [==============================] - 1s 5ms/sample - loss: 0.4629 - acc: 0.8022 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "[0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1.]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "train data: 268 validation: 22\n",
      "Dont buys: 134, buys: 134\n",
      "TEST Dont buys: 11, buys: 11\n",
      "Train on 268 samples, validate on 22 samples\n",
      "Epoch 1/10\n",
      "268/268 [==============================] - 11s 40ms/sample - loss: 1.1147 - acc: 0.4888 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "268/268 [==============================] - 1s 3ms/sample - loss: 0.8623 - acc: 0.5970 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "268/268 [==============================] - 2s 6ms/sample - loss: 0.7162 - acc: 0.6604 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "268/268 [==============================] - 1s 4ms/sample - loss: 0.7222 - acc: 0.6045 - val_loss: 0.6939 - val_acc: 0.5455\n",
      "Epoch 5/10\n",
      "268/268 [==============================] - 1s 3ms/sample - loss: 0.6486 - acc: 0.6381 - val_loss: 0.6939 - val_acc: 0.5455\n",
      "Epoch 6/10\n",
      "268/268 [==============================] - 1s 4ms/sample - loss: 0.5502 - acc: 0.7090 - val_loss: 0.6943 - val_acc: 0.4545\n",
      "Epoch 7/10\n",
      "268/268 [==============================] - 1s 5ms/sample - loss: 0.5850 - acc: 0.7015 - val_loss: 0.6946 - val_acc: 0.4091\n",
      "Epoch 8/10\n",
      "268/268 [==============================] - 1s 5ms/sample - loss: 0.5375 - acc: 0.7239 - val_loss: 0.6948 - val_acc: 0.4091\n",
      "Epoch 9/10\n",
      "268/268 [==============================] - 1s 4ms/sample - loss: 0.4820 - acc: 0.7873 - val_loss: 0.6951 - val_acc: 0.4545\n",
      "Epoch 10/10\n",
      "268/268 [==============================] - 2s 6ms/sample - loss: 0.4616 - acc: 0.7836 - val_loss: 0.6949 - val_acc: 0.4091\n",
      "[1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "[1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1]\n",
      "train data: 268 validation: 22\n",
      "Dont buys: 134, buys: 134\n",
      "TEST Dont buys: 11, buys: 11\n",
      "Train on 268 samples, validate on 22 samples\n",
      "Epoch 1/10\n",
      "268/268 [==============================] - 12s 43ms/sample - loss: 0.7467 - acc: 0.5410 - val_loss: 0.6933 - val_acc: 0.4091\n",
      "Epoch 2/10\n",
      "268/268 [==============================] - 1s 3ms/sample - loss: 0.6643 - acc: 0.6530 - val_loss: 0.6933 - val_acc: 0.5455\n",
      "Epoch 3/10\n",
      "268/268 [==============================] - 2s 6ms/sample - loss: 0.6164 - acc: 0.6530 - val_loss: 0.6936 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "268/268 [==============================] - 2s 6ms/sample - loss: 0.6165 - acc: 0.6679 - val_loss: 0.6938 - val_acc: 0.5455\n",
      "Epoch 5/10\n",
      "268/268 [==============================] - 2s 6ms/sample - loss: 0.5210 - acc: 0.7052 - val_loss: 0.6942 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "268/268 [==============================] - 2s 7ms/sample - loss: 0.5709 - acc: 0.6754 - val_loss: 0.6943 - val_acc: 0.4545\n",
      "Epoch 7/10\n",
      "268/268 [==============================] - 2s 6ms/sample - loss: 0.5300 - acc: 0.7276 - val_loss: 0.6941 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "268/268 [==============================] - 1s 5ms/sample - loss: 0.4992 - acc: 0.7537 - val_loss: 0.6938 - val_acc: 0.5455\n",
      "Epoch 9/10\n",
      "268/268 [==============================] - 1s 3ms/sample - loss: 0.4868 - acc: 0.7575 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "268/268 [==============================] - 1s 2ms/sample - loss: 0.4559 - acc: 0.7836 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "[1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1.]\n",
      "[1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1]\n",
      "train data: 268 validation: 22\n",
      "Dont buys: 134, buys: 134\n",
      "TEST Dont buys: 11, buys: 11\n",
      "Train on 268 samples, validate on 22 samples\n",
      "Epoch 1/10\n",
      "268/268 [==============================] - 14s 51ms/sample - loss: 0.9324 - acc: 0.5149 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "268/268 [==============================] - 1s 3ms/sample - loss: 0.6462 - acc: 0.6418 - val_loss: 0.6936 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "268/268 [==============================] - 1s 5ms/sample - loss: 0.7204 - acc: 0.5933 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "268/268 [==============================] - 1s 4ms/sample - loss: 0.5960 - acc: 0.7015 - val_loss: 0.6938 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "268/268 [==============================] - 1s 3ms/sample - loss: 0.6467 - acc: 0.6567 - val_loss: 0.6942 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "268/268 [==============================] - 1s 4ms/sample - loss: 0.5639 - acc: 0.7313 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "268/268 [==============================] - 1s 3ms/sample - loss: 0.5202 - acc: 0.7313 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "268/268 [==============================] - 2s 6ms/sample - loss: 0.5195 - acc: 0.7761 - val_loss: 0.6939 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "268/268 [==============================] - 1s 4ms/sample - loss: 0.4896 - acc: 0.7687 - val_loss: 0.6939 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "268/268 [==============================] - 2s 9ms/sample - loss: 0.4799 - acc: 0.7351 - val_loss: 0.6941 - val_acc: 0.5000\n",
      "[0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "train data: 268 validation: 22\n",
      "Dont buys: 134, buys: 134\n",
      "TEST Dont buys: 11, buys: 11\n",
      "Train on 268 samples, validate on 22 samples\n",
      "Epoch 1/10\n",
      "268/268 [==============================] - 15s 57ms/sample - loss: 0.8487 - acc: 0.5858 - val_loss: 0.6923 - val_acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "268/268 [==============================] - 1s 4ms/sample - loss: 0.6776 - acc: 0.6269 - val_loss: 0.6921 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "268/268 [==============================] - 2s 6ms/sample - loss: 0.6784 - acc: 0.6530 - val_loss: 0.6920 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "268/268 [==============================] - 1s 3ms/sample - loss: 0.6339 - acc: 0.6754 - val_loss: 0.6924 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "268/268 [==============================] - 1s 5ms/sample - loss: 0.6423 - acc: 0.6455 - val_loss: 0.6928 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "268/268 [==============================] - 1s 5ms/sample - loss: 0.5226 - acc: 0.7575 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "268/268 [==============================] - 2s 6ms/sample - loss: 0.4539 - acc: 0.7799 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "268/268 [==============================] - 1s 5ms/sample - loss: 0.4325 - acc: 0.7910 - val_loss: 0.6941 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "268/268 [==============================] - 1s 4ms/sample - loss: 0.4376 - acc: 0.7985 - val_loss: 0.6942 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "268/268 [==============================] - 1s 3ms/sample - loss: 0.4739 - acc: 0.7537 - val_loss: 0.6947 - val_acc: 0.5000\n",
      "[1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1.]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 7                   # 1 week of data\n",
    "FUTURE_PERIOD_PREDICT = 2     # 2 days in the future\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "data_df = pd.DataFrame()\n",
    "main_df = pd.DataFrame()\n",
    "assets = [\"BTC\", \"LTC\", \"ETH\", \"AMZN\", \"GOOGL\", \"FB\"]\n",
    "\n",
    "# Main loop to plug each asset into the machine learning model\n",
    "for asset in assets:\n",
    "    NAME = f\"{asset}-{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
    "    \n",
    "    # Sub-loop all data to build dataframe; add csv columns into main_df\n",
    "    for asset in assets:\n",
    "        dataset = f\"Data/{asset}.csv\"\n",
    "    \n",
    "        df = pd.read_csv(dataset)\n",
    "        df.rename(columns={\"close\": f\"{asset}_close\", \"volume\": f\"{asset}_volume\"}, inplace=True)\n",
    "    \n",
    "        df.set_index('date', inplace=True)\n",
    "        df = df[[f\"{asset}_close\", f\"{asset}_volume\"]]\n",
    "    \n",
    "        if len(main_df) == 0:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df)\n",
    "        \n",
    "\n",
    "    main_df['future'] = main_df[f\"{asset}_close\"].shift(-FUTURE_PERIOD_PREDICT)\n",
    "    main_df['target'] = list(map(classify, main_df[f\"{asset}_close\"], main_df[\"future\"]))\n",
    "    # print(main_df[[f\"{COIN_TO_PREDICT}_close\",\"future\",\"target\"]].head(10))\n",
    "\n",
    "\n",
    "    # Sort dataframes by date and set last 10% of dates as a variable\n",
    "    times = sorted(main_df.index.values)\n",
    "    last_10pct = times[-int(0.1*len(times))]\n",
    "\n",
    "\n",
    "    # Split first 90% of data for training data and last 10% as test data\n",
    "    validate_main_df = main_df[(main_df.index >= last_10pct)]\n",
    "    main_df = main_df[(main_df.index < last_10pct)]\n",
    "\n",
    "    train_x, train_y = preprocess_df(main_df)\n",
    "    validate_x, validate_y = preprocess_df(validate_main_df)\n",
    "\n",
    "    print(f\"train data: {len(train_x)} validation: {len(validate_x)}\")\n",
    "    print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
    "    print(f\"TEST Dont buys: {validate_y.count(0)}, buys: {validate_y.count(1)}\")\n",
    "\n",
    "    # Initialize sequential model and add a few layers for better accuracy\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LSTM(128, input_shape=(train_x.shape[1:])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Final layer has 2 output nodes for binary classification\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                 optimizer=opt,\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    # Visualize model training\n",
    "    tensorboard = TensorBoard(log_dir= f'logs/{NAME}')\n",
    "\n",
    "    filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"\n",
    "    checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max'))\n",
    "\n",
    "    history = model.fit(train_x, train_y, batch_size=BATCH_SIZE,\n",
    "                        epochs=EPOCHS, validation_data=(validate_x, validate_y),\n",
    "                       callbacks=[tensorboard, checkpoint])\n",
    "\n",
    "\n",
    "\n",
    "    # Output predictions: 1=buy, 0=sell     \n",
    "    predictions = np.argmax(model.predict(validate_x), axis=1)\n",
    "\n",
    "    # Actual test values to compare against predictions\n",
    "    actual = np.array(validate_y).ravel()\n",
    "    \n",
    "    print(actual)\n",
    "    print(predictions)\n",
    "    \n",
    "#     predictions = pd.DataFrame({f'{asset}': pred_x})\n",
    "    \n",
    "#     if len(main_df) == 0:\n",
    "#         data_df = ({'actual values': actual})\n",
    "#         data_df.join(predictions)\n",
    "#     else:\n",
    "#         data_df = data_df.join(predictions)\n",
    "    \n",
    "\n",
    "    # Clears dataframes before next iteration through loop\n",
    "#     predictions = predictions.iloc[0:0]\n",
    "    main_df = main_df.iloc[0:0]\n",
    "    df = df.iloc[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

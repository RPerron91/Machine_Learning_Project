{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 7                   # 1 week of data\n",
    "FUTURE_PERIOD_PREDICT = 2     # 2 days in the future\n",
    "COIN_TO_PREDICT = \"LTC\"\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "NAME = f\"{COIN_TO_PREDICT}-{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify predictions as boolean values\n",
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Preprocess values and remove future predictions column\n",
    "def preprocess_df(df):\n",
    "    df = df.drop('future', 1)\n",
    "    \n",
    "    # Drop non-numeric rows and change prices to percentage change values, then scale each column\n",
    "    for col in df.columns:\n",
    "        if col != \"target\":\n",
    "            df[col] = df[col].pct_change()\n",
    "            df.dropna(inplace=True)\n",
    "            df[col] = preprocessing.scale(df[col].values)\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Sort values into sequential chunks before randomizing for more accurate model predictions\n",
    "    sequential_data = []\n",
    "    prev_days = deque(maxlen=SEQ_LEN)\n",
    "    \n",
    "    for i in df.values:\n",
    "        prev_days.append([n for n in i[:-1]])\n",
    "        if len(prev_days) == SEQ_LEN:\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])\n",
    "            \n",
    "    # Randomize the sequence\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    buys = []\n",
    "    sells = []\n",
    "    \n",
    "    # If future price is greater than current price, buy, if future price less than current price, sell\n",
    "    for seq, target in sequential_data:\n",
    "        if target == 0:\n",
    "            sells.append([seq, target])\n",
    "        elif target == 1:\n",
    "            buys.append([seq, target])\n",
    "    \n",
    "    # Randomize each buy or sell\n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "    \n",
    "    # Balance buys/sells by creating equal length lists for buys/sells\n",
    "    # Use the lowest length value of the two lists as constraints\n",
    "    lower = min(len(buys), len(sells))\n",
    "    \n",
    "    buys = buys[:lower]\n",
    "    sells = sells[:lower]\n",
    "    \n",
    "    \n",
    "    sequential_data = buys+sells\n",
    "    random.shuffle(sequential_data)\n",
    "    \n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "        \n",
    "    return np.array(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            LTC_close    future  target\n",
      "date                                   \n",
      "2017-02-10   3.779899  3.719400       0\n",
      "2017-02-11   3.757000  3.790000       1\n",
      "2017-02-12   3.719400  3.783455       1\n",
      "2017-02-13   3.790000  3.904180       1\n",
      "2017-02-14   3.783455  3.860000       1\n",
      "2017-02-15   3.904180  3.818332       0\n",
      "2017-02-16   3.860000  3.721817       0\n",
      "2017-02-17   3.818332  3.718663       0\n",
      "2017-02-18   3.721817  3.710000       0\n",
      "2017-02-19   3.718663  3.856600       1\n"
     ]
    }
   ],
   "source": [
    "main_df = pd.DataFrame()\n",
    "\n",
    "# Loop through all data, add csv columns into main_df\n",
    "altcoins = [\"BTC\", \"LTC\", \"ETH\", \"ETC\", \"XMR\", \"XRP\"]\n",
    "for coin in altcoins:\n",
    "    dataset = f\"Data/Altcoins/{coin}.csv\"\n",
    "    \n",
    "    df = pd.read_csv(dataset)\n",
    "    df.rename(columns={\"close\": f\"{coin}_close\", \"volume\": f\"{coin}_volume\"}, inplace=True)\n",
    "    \n",
    "    df.set_index('date', inplace=True)\n",
    "    df = df[[f\"{coin}_close\", f\"{coin}_volume\"]]\n",
    "    \n",
    "    if len(main_df) == 0:\n",
    "        main_df = df\n",
    "    else:\n",
    "        main_df = main_df.join(df)\n",
    "        \n",
    "main_df['future'] = main_df[f\"{COIN_TO_PREDICT}_close\"].shift(-FUTURE_PERIOD_PREDICT)\n",
    "main_df['target'] = list(map(classify, main_df[f\"{COIN_TO_PREDICT}_close\"], main_df[\"future\"]))\n",
    "print(main_df[[f\"{COIN_TO_PREDICT}_close\",\"future\",\"target\"]].head(10))\n",
    "\n",
    "times = sorted(main_df.index.values)\n",
    "last_10pct = times[-int(0.1*len(times))]\n",
    "\n",
    "validate_main_df = main_df[(main_df.index >= last_10pct)]\n",
    "main_df = main_df[(main_df.index < last_10pct)]\n",
    "\n",
    "train_x, train_y = preprocess_df(main_df)\n",
    "validate_x, validate_y = preprocess_df(validate_main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 620 validation: 50\n",
      "Dont buys: 310, buys: 310\n",
      "TEST Dont buys: 25, buys: 25\n"
     ]
    }
   ],
   "source": [
    "print(f\"train data: {len(train_x)} validation: {len(validate_x)}\")\n",
    "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
    "print(f\"TEST Dont buys: {validate_y.count(0)}, buys: {validate_y.count(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 620 samples, validate on 50 samples\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "620/620 [==============================] - 6s 10ms/sample - loss: 0.8676 - acc: 0.5129 - val_loss: 0.6933 - val_acc: 0.4200\n",
      "Epoch 2/10\n",
      "620/620 [==============================] - 1s 1ms/sample - loss: 0.7052 - acc: 0.6097 - val_loss: 0.6936 - val_acc: 0.4200\n",
      "Epoch 3/10\n",
      "620/620 [==============================] - 1s 2ms/sample - loss: 0.6953 - acc: 0.6032 - val_loss: 0.6930 - val_acc: 0.4800\n",
      "Epoch 4/10\n",
      "620/620 [==============================] - 1s 2ms/sample - loss: 0.6274 - acc: 0.6694 - val_loss: 0.6930 - val_acc: 0.3800\n",
      "Epoch 5/10\n",
      "620/620 [==============================] - 1s 1ms/sample - loss: 0.6070 - acc: 0.6516 - val_loss: 0.6917 - val_acc: 0.4600\n",
      "Epoch 6/10\n",
      "620/620 [==============================] - 1s 1ms/sample - loss: 0.5685 - acc: 0.6677 - val_loss: 0.6897 - val_acc: 0.5400\n",
      "Epoch 7/10\n",
      "620/620 [==============================] - 1s 1ms/sample - loss: 0.5649 - acc: 0.7177 - val_loss: 0.6886 - val_acc: 0.5200\n",
      "Epoch 8/10\n",
      "620/620 [==============================] - 1s 2ms/sample - loss: 0.5443 - acc: 0.7032 - val_loss: 0.6886 - val_acc: 0.5400\n",
      "Epoch 9/10\n",
      "620/620 [==============================] - 1s 866us/sample - loss: 0.5294 - acc: 0.7226 - val_loss: 0.6856 - val_acc: 0.6200\n",
      "Epoch 10/10\n",
      "620/620 [==============================] - 1s 2ms/sample - loss: 0.5408 - acc: 0.7226 - val_loss: 0.6845 - val_acc: 0.5800\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "             optimizer=opt,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "tensorboard = TensorBoard(log_dir= f'logs/{NAME}')\n",
    "\n",
    "filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"\n",
    "checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max'))\n",
    "\n",
    "history = model.fit(train_x, train_y, batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS, validation_data=(validate_x, validate_y),\n",
    "                   callbacks=[tensorboard, checkpoint])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(validate_x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(validate_y).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
